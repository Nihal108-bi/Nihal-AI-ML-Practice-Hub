{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nihal108-bi/Nihal-AI-ML-Practice-Hub/blob/main/HugginFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4f_9ixqQQaY"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "API_TOKEN = userdata.get('Huggingface')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-7b\""
      ],
      "metadata": {
        "id": "4aMf1HdOR4t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_TOKEN"
      ],
      "metadata": {
        "id": "-zQjCmh-SdvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "o9817eydSfsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GEMMA MODEL API CALLING USING HUGGING FACE"
      ],
      "metadata": {
        "id": "uk3X8Gs4VzrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_URL = \"https://api-inference.huggingface.co/models/google/gemma-7b\""
      ],
      "metadata": {
        "id": "ud5FHgZcSpEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query(payload):\n",
        "  response = requests.post(API_URL, headers = {\"Authorization\":f\"Bearer {API_TOKEN}\"}, json = payload)\n",
        "  print(response)\n",
        "  return response.json()"
      ],
      "metadata": {
        "id": "NM6oDrpOS5GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "    \"inputs\": \"Can you tell me what is the capital of france\"\n",
        "}"
      ],
      "metadata": {
        "id": "WM43TugfTXxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = query(payload)\n"
      ],
      "metadata": {
        "id": "0RD_7VYbTjCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "r-S05qp5TrBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "ttDIJRgLTtxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(output[0]['generated_text']))"
      ],
      "metadata": {
        "id": "lt-ymmLXT4_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MISTRAL MODEL API CALLING USING HUGGIN FACE"
      ],
      "metadata": {
        "id": "p8_PxWuGVttu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MISTRAL_API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\""
      ],
      "metadata": {
        "id": "3qdbee4uUBb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "    \"inputs\": \"Tell me a joke\"\n",
        "}"
      ],
      "metadata": {
        "id": "IoVttL3GUoQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query(payload, API_URL):\n",
        "  response = requests.post(API_URL, headers = {\"Authorization\":f\"Bearer {API_TOKEN}\"}, json = payload)\n",
        "  print(response)\n",
        "  return response.json()"
      ],
      "metadata": {
        "id": "IYX2W3FXyqU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mistral_output = query(payload, MISTRAL_API_URL)"
      ],
      "metadata": {
        "id": "3C3zpNHjUuSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\"\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer YOUR_HUGGINGFACE_API_TOKEN\"\n",
        "}\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\n",
        "    # Check if response is OK and not empty\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error:\", response.status_code, response.text)\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        return response.json()\n",
        "    except requests.exceptions.JSONDecodeError:\n",
        "        print(\"Could not decode JSON. Raw response:\")\n",
        "        print(response.text)\n",
        "        return None\n",
        "\n",
        "output = query({\"inputs\": \"Tell me a joke\"})\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "R5dV9S8_wMqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(mistral_output[0]['generated_text']))"
      ],
      "metadata": {
        "id": "GbdC6WbMVCpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2JUqkA7YVIOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ZEPHYR MODEL API CALLING USING HUGGING FACE"
      ],
      "metadata": {
        "id": "SJ-vSTj8V4Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ZEPHYR_API_URL = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n"
      ],
      "metadata": {
        "id": "LiF3YMrWV7xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "    \"inputs\": \"Can you tell me what is the LLM\"\n",
        "}"
      ],
      "metadata": {
        "id": "I4aNo_ZxWCIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zephyr_output = query(payload, ZEPHYR_API_URL)"
      ],
      "metadata": {
        "id": "EKIBHLzEWII7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(zephyr_output[0]['generated_text']))"
      ],
      "metadata": {
        "id": "8Sxvrk-vWMwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C9RHe5wMWSdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SINGLE FUNCTION"
      ],
      "metadata": {
        "id": "hG2fpE6GWcjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def queryHF(prompt, model_name):\n",
        "  api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
        "  print(api_url)\n",
        "  payload = {\n",
        "      \"inputs\": prompt\n",
        "  }\n",
        "  response = requests.post(api_url, headers = {\"Authorization\":f\"Bearer {API_TOKEN}\"}, json = payload)\n",
        "  print(response)\n",
        "  return display(Markdown(response.json()[0]['generated_text']))"
      ],
      "metadata": {
        "id": "GIJQQs2RWeEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queryHF(\"what is Gemini\", \"Gemini/GemmaX2-28-9B-v0.1-Q2_K-GGUF\")\n"
      ],
      "metadata": {
        "id": "1pp6qeUsWp0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queryHF(\"what is chatgpt\", \"HuggingFaceH4/zephyr-7b-beta\")"
      ],
      "metadata": {
        "id": "0pI3-dOb1o3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HG34Il59XLmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HUGGING FACE FINE TUNE MODEL"
      ],
      "metadata": {
        "id": "Oh-SClROg43G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A52k_GRvg7xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ANh6AmOWhdGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CODE FLOW FOR HUGGING FACE (in general)\n",
        "\n",
        "- There are few steps to start using hugging face\n",
        "- First install the hugging face transformers and datasets library. These libraries contains modules for interacting with models (ML, DL models) and Datasets\n",
        "- A ml framework such pytorch will also be needed for working with ml models and transformer packages."
      ],
      "metadata": {
        "id": "8X8-ujNdhiP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sVS8OfsQiTvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NF0PMEwOiZW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0TY110QipY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. To navigate the hugging face hub on your colab notebook you need to install huggingface_hub python package\n",
        "2. Then we need to import HfApi from hugginface_hub create an instance of the api to communicate with the hugging face using pyton"
      ],
      "metadata": {
        "id": "YI9wOtJ3jCia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GnFR-Oo2jX_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "51_CCl6Xjfpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(api.list_models())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BSDSBzKjjpwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api = HfApi()\n",
        "models = api.list_models(\n",
        "    sort = 'downloads',\n",
        "    direction = -1,\n",
        "    limit = 5\n",
        ")\n",
        "\n",
        "modelList = list(models)\n",
        "\n",
        "print(modelList)\n"
      ],
      "metadata": {
        "id": "LF__Kq2pj2yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFGMEFmglmRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inside transformers we use AutoModel[link text](https://)"
      ],
      "metadata": {
        "id": "ayMez5melxh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "modelid = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "model = AutoModel.from_pretrained(modelid)\n",
        "\n",
        "model.save_pretrained(save_directory= f\"models/{modelid}\")"
      ],
      "metadata": {
        "id": "RP82tcarl5f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "RwjX2k_omu_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset_builder\n",
        "\n",
        "data_builder = load_dataset_builder(\"fka/awesome-chatgpt-prompts\")\n"
      ],
      "metadata": {
        "id": "MAaVQr4JmiJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_builder.info.features"
      ],
      "metadata": {
        "id": "855y8-ULnG4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_builder.info.dataset_size"
      ],
      "metadata": {
        "id": "4tYAk8t_nLbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names\n",
        "\n",
        "configs = get_dataset_config_names(\"wikipedia\")\n",
        "print(configs)\n"
      ],
      "metadata": {
        "id": "4cdlaA0rnby9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset('imdb', split=\"train\")"
      ],
      "metadata": {
        "id": "Cq_V87DUnrQu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "usrQ8MaSoUul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset_builder\n",
        "dataset = load_dataset('imdb', split=\"train\")\n",
        "\n",
        "dataset.save_to_disk('imdb_dataset')"
      ],
      "metadata": {
        "id": "Op6OoD_eohgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-0hNSLbYo5ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIPELINES"
      ],
      "metadata": {
        "id": "2vKbakPsw2Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HFZKH5mw3_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# building pipelines with Hugging face\n",
        "- There are two ways that we can use to create pipelines\n",
        "- one uses transformers library and specifically, automodel class from the transformers library\n",
        "- Second way is using high level helper frameworks/library provided by huggingface"
      ],
      "metadata": {
        "id": "EUwzH2h1xCzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features of Automodel class\n",
        "- general class with multiple uses.\n",
        "- It is used to interact with models\n",
        "- It also has tokenizers\n",
        "- availability of configurations\n",
        "- processors\n",
        "- feature extractor\n",
        "- flexible and direct\n",
        "- more control for various tasks\n",
        "- We have individual subclasses for each task"
      ],
      "metadata": {
        "id": "CHEaZaH6xh6P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4zfYYTSxawC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline wrapper library (2nd way of creating pipelines)\n",
        "- Pipeline class is also part of transformer library\n",
        "- contains all task specific steps\n",
        "- best for quickly performing tasks\n"
      ],
      "metadata": {
        "id": "pa8F-IcIys5f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3blTC47EzLNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automodel class implementation examples**"
      ],
      "metadata": {
        "id": "L8jO4DPMzPhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VP0vRpoczUM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "3dfiR0XBzxS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMmXXiACz_3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The pipeline class/module**"
      ],
      "metadata": {
        "id": "nV0S8uGC0KfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "my_pipeline = pipeline(task=\"text-classification\")\n"
      ],
      "metadata": {
        "id": "cwKUwmfQ0Ms_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_pipeline = pipeline(model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")"
      ],
      "metadata": {
        "id": "YUKinml00bEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_pipeline"
      ],
      "metadata": {
        "id": "jhxm3MVj0sJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_pipeline = pipeline(task=\"text-classification\", model = \"distilbert-base-uncased-finetuned-sst-2-english\")"
      ],
      "metadata": {
        "id": "gisJuxrC0uns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_prompt = \"I am very happy\"\n",
        "\n",
        "my_pipeline(input_prompt)"
      ],
      "metadata": {
        "id": "R2_nceHQ1FFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_prompt = \"he rain poured relentlessly, mirroring the despair that weighed heavily on my heart. Everything seemed bleak and shrouded in a gloomy haze. A sense of unease settled deep within me, a knot of anxiety tightening with each passing moment. Disappointment gnawed at my soul, leaving behind a bitter taste of regret. The world felt cold and uncaring, as if it had conspired against my every hope and dream.\""
      ],
      "metadata": {
        "id": "mH7F7AKu1QGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_pipeline(negative_prompt)"
      ],
      "metadata": {
        "id": "OUm4Smav1fyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(negative_prompt)"
      ],
      "metadata": {
        "id": "GQngI4d41iTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "input = \"Hello ,,, !! how ARE you\"\n",
        "\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "gpt_tokens = gpt_tokenizer.tokenize(input)\n",
        "\n",
        "print(gpt_tokens)"
      ],
      "metadata": {
        "id": "iEGwr0wg1se3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"michelecafagna26/gpt2-medium-finetuned-sst2-sentiment\")\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"michelecafagna26/gpt2-medium-finetuned-sst2-sentiment\")\n",
        "\n",
        "inputs = tokenizer(\"I hate it\", return_tensors=\"pt\")\n",
        "\n",
        "model(**inputs).logits.argmax(axis=1)\n",
        "\n",
        "# 1: Positive, 0: Negative\n",
        "# Output: tensor([1])\n"
      ],
      "metadata": {
        "id": "wxAqsslr4lXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VL9A70gK4TtW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}